#!/usr/bin/python
# Split pre-parsed exactEarth position-referenced AIS data (Postgres Export) 
# into basic movement data files on message type group.

from glob import glob
import sys
import os
import time

# Usage string for the script.
usage_string = "Usage: split_eE_AIS_pre_tracks_NEMES.py outputdirectory outputfilenameprefix inputfilename1 [inputfilename2 ...] \n\nSplits pre-parsed position-referenced eE AIS records (Postgres Export) into sub files by message type, then mmsi pre track creation for the NEMES project.\n"

# If at least two arguments are not provided, display an usage message.
if (len(sys.argv) < 4):
    print usage_string
    quit()
    
# retrieve the output directory and filename prefix
outdirectory = sys.argv[1]
out_filename_prefix = sys.argv[2]

# Define an array of output filenames, based on the provided prefix, to store the parsed results.
out_filename_array = [out_filename_prefix + "_1_2_3.csv", out_filename_prefix + "_18_19.csv", out_filename_prefix + "_other.csv"]

# Check each potential output file for existence before running.
for outfile_index in range(len(out_filename_array) - 1):
    if os.path.exists(out_filename_array[outfile_index]):
        print "Error, output file exists: (" + out_filename_array[outfile_index] +  ") aborting."
        quit()
        
# Open all output files required.
out_message_records = []

for outfile_index in range(len(out_filename_array)):
    try:
        #out_message_records[outfile_index] = open(out_filename_array[outfile_index], 'w')
        out_message_records.append(open(out_filename_array[outfile_index], 'w'))
    except IOError:
        print "Error opening output file: " + out_filename_array[outfile_index] + "\n"
        quit()

# Print a header line for each of the message type groups to be extracted from the eE AIS data.

#1_2_3
out_message_records[0].write("ext_timestamp,msgid,mmsi,nav_stat,sog,cog,tr_hdg,lat,lon,pos_acc\n");

#18_19 
out_message_records[1].write("ext_timestamp,msgid,mmsi,nav_stat,sog,cog,tr_hdg,lat,lon,pos_acc\n");
    
#other
out_message_records[2].write("Field set depends on message type, see split_eE_AIS_for_PG_base_table_w_parsing.py \n")

# Process each input file reference passed as input.
for infile_index in range(len(sys.argv) - 3):

    # Attempt wildcard expansion on any input file specified.
    for in_filename in glob(sys.argv[(3 + infile_index)]):
    
        print("Processing: " + in_filename)
        
        with open(in_filename,'r') as in_vessel_records:
        
            # Reset a counter into the input file.
            in_line_counter = 0
        
            for line in in_vessel_records:
                
                # Split the input line on the "outer" tab-character based tokenization (generated by Postgres).
                #UNQ_ID\tMMSI\tLON\tLAT\tDATETIME\tMSG_ID\tPARSEERR\tAISMSG
                tabdelline = line.split('\t');
                input_msg_type = tabdelline[5]
                
                # Tokenize the record data within the 7th field on the basis of pipe-character.
                pipetokenizedline = tabdelline[7].strip().split('|')
                
                # Output tokenized fields according to the message type observed.
                #1_2_3                
                if(input_msg_type in ("1", "2", "3")):
               
                    # Old -  ext_timestamp,repeat,msgid,mmsi,nav_stat,sog,cog,tr_hdg,lat,lon,pos_acc,sh_type,sh_name
                    # Old - out_message_records[0].write(pipetokenizedline[3] + "," + pipetokenizedline[2] + "," + pipetokenizedline[1] + "," + pipetokenizedline[0] + "," + pipetokenizedline[13] + "," + pipetokenizedline[15] + "," + pipetokenizedline[19] + "," + pipetokenizedline[20] + "," + pipetokenizedline[18] + "," + pipetokenizedline[17] + "," + pipetokenizedline[16] + "," + "" + "," + "" + "\n")

                    # ext_timestamp,msgid,mmsi,nav_stat,sog,cog,tr_hdg,lat,lon,pos_acc
                    out_message_records[0].write(pipetokenizedline[3] + "," + pipetokenizedline[1] + "," + pipetokenizedline[0] + "," + pipetokenizedline[13] + "," + pipetokenizedline[15] + "," + pipetokenizedline[19] + "," + pipetokenizedline[20] + "," + pipetokenizedline[18] + "," + pipetokenizedline[17] + "," + pipetokenizedline[16] + "\n")


                #18_19 
                elif(input_msg_type in ("18", "19")):

                    # Pre-parser ordering of fields from raw AIS type 18_19:
                    
                    #Old - ext_timestamp,repeat,msgid,mmsi,nav_stat,sog,cog,tr_hdg,lat,lon,pos_acc,sh_type,sh_name
                    #Old - 3,2,1,0,none,19,23,24,22,21,20,14,13
                    #Old - out_message_records[1].write(pipetokenizedline[3] + "," + pipetokenizedline[2] + "," + pipetokenizedline[1] + "," + pipetokenizedline[0] + "," + "" + "," + pipetokenizedline[19] + "," + pipetokenizedline[23] + "," + pipetokenizedline[24] + "," + pipetokenizedline[22] + "," + pipetokenizedline[21] + "," + pipetokenizedline[20] + "," + pipetokenizedline[14] + "," + pipetokenizedline[13] + "\n")

                    #ext_timestamp,msgid,mmsi,nav_stat,sog,cog,tr_hdg,lat,lon,pos_acc
                    #3,1,0,none,19,23,24,22,21,20
                    out_message_records[1].write(pipetokenizedline[3] + "," + pipetokenizedline[1] + "," + pipetokenizedline[0] + "," + "" + "," + pipetokenizedline[19] + "," + pipetokenizedline[23] + "," + pipetokenizedline[24] + "," + pipetokenizedline[22] + "," + pipetokenizedline[21] + "," + pipetokenizedline[20] + "," + "\n")
                    
                #other - write out input line as received.
                else:
                
                    out_message_records[2].write(line)
                    
                # Increment the current input line counter.
                in_line_counter += 1
                    
# Close / flush all output files required.
for outfile_index in range(len(out_message_records)):  
    out_message_records[outfile_index].close()
    
# Run through the 1,2,3 and 18,19 parsed files (all but last in array out_filename_array, 
# allocating the records within to new output files on the basis of mmsi.
for data_index in range(len(out_filename_array) - 1):
    
    print("Processing: " + out_filename_array[(data_index)])

    with open(out_filename_array[data_index],'r') as in_parsed_sing_AIS_underway:

        for line in in_parsed_sing_AIS_underway:
            
            tokenizedline = line.split(',')

            # Verify that the incoming line has the correct number of tokens.
            if(len(tokenizedline) > 8):
                
                mmsi = tokenizedline[2];
                
                try:
                    outfile = open(outdirectory  + "/" + mmsi + ".txt", 'a')
                except IOError:
                    print "Error opening file: " + outdirectory + mmsi + "\n"
                    quit()
                    
                outfile.write(line)
                outfile.close
            else:
                
                print("Error, incorrect number of tokens:" + str(len(tokenizedline)) + "Line:" + line + "\n")

